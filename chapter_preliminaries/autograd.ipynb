{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b4873f83",
      "metadata": {
        "origin_pos": 0,
        "id": "b4873f83"
      },
      "source": [
        "# 自动微分\n",
        ":label:`sec_autograd`\n",
        "\n",
        "正如 :numref:`sec_calculus`中所说，求导是几乎所有深度学习优化算法的关键步骤。\n",
        "虽然求导的计算很简单，只需要一些基本的微积分。\n",
        "但对于复杂的模型，手工进行更新是一件很痛苦的事情（而且经常容易出错）。\n",
        "\n",
        "深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。\n",
        "实际中，根据设计好的模型，系统会构建一个*计算图*（computational graph），\n",
        "来跟踪计算是哪些数据通过哪些操作组合起来产生输出。\n",
        "自动微分使系统能够随后反向传播梯度。\n",
        "这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。\n",
        "\n",
        "## 一个简单的例子\n",
        "\n",
        "作为一个演示例子，(**假设我们想对函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于列向量$\\mathbf{x}$求导**)。\n",
        "首先，我们创建变量`x`并为其分配一个初始值。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "98cd8a9e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:31.627945Z",
          "iopub.status.busy": "2023-08-18T07:07:31.627424Z",
          "iopub.status.idle": "2023-08-18T07:07:32.686372Z",
          "shell.execute_reply": "2023-08-18T07:07:32.685559Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "98cd8a9e",
        "outputId": "6cfb8120-e27c-4569-efa4-2bd2cd1ab910",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.arange(4.0)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec430520",
      "metadata": {
        "origin_pos": 5,
        "id": "ec430520"
      },
      "source": [
        "[**在我们计算$y$关于$\\mathbf{x}$的梯度之前，需要一个地方来存储梯度。**]\n",
        "重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
        "因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。\n",
        "注意，一个标量函数关于向量$\\mathbf{x}$的梯度是向量，并且与$\\mathbf{x}$具有相同的形状。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e27a5df4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.690633Z",
          "iopub.status.busy": "2023-08-18T07:07:32.689882Z",
          "iopub.status.idle": "2023-08-18T07:07:32.694159Z",
          "shell.execute_reply": "2023-08-18T07:07:32.693367Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "e27a5df4"
      },
      "outputs": [],
      "source": [
        "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
        "x.grad  # 默认值是None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd993524",
      "metadata": {
        "origin_pos": 10,
        "id": "bd993524"
      },
      "source": [
        "(**现在计算$y$。**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4c3f80b7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.698006Z",
          "iopub.status.busy": "2023-08-18T07:07:32.697167Z",
          "iopub.status.idle": "2023-08-18T07:07:32.705385Z",
          "shell.execute_reply": "2023-08-18T07:07:32.704593Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "4c3f80b7",
        "outputId": "ca1d14c6-3efe-4814-dbd6-5b87956b2a8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "id": "1qHkGfbYFvDD",
        "outputId": "cd56b7d3-495d-45be-9e6e-2bcfc98e9571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1qHkGfbYFvDD",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35523dbc",
      "metadata": {
        "origin_pos": 15,
        "id": "35523dbc"
      },
      "source": [
        "`x`是一个长度为4的向量，计算`x`和`x`的点积，得到了我们赋值给`y`的标量输出。\n",
        "接下来，[**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**]，并打印这些梯度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a1c3a419",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.708698Z",
          "iopub.status.busy": "2023-08-18T07:07:32.708196Z",
          "iopub.status.idle": "2023-08-18T07:07:32.713924Z",
          "shell.execute_reply": "2023-08-18T07:07:32.713091Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "a1c3a419",
        "outputId": "37cdaa30-785c-492f-8f10-5323fc21daf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "y.backward()   # y=2x^T*x   df/dx=4x\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca6a271",
      "metadata": {
        "origin_pos": 20,
        "id": "dca6a271"
      },
      "source": [
        "函数$y=2\\mathbf{x}^{\\top}\\mathbf{x}$关于$\\mathbf{x}$的梯度应为$4\\mathbf{x}$。\n",
        "让我们快速验证这个梯度是否计算正确。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b8493d0a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.718858Z",
          "iopub.status.busy": "2023-08-18T07:07:32.718156Z",
          "iopub.status.idle": "2023-08-18T07:07:32.724091Z",
          "shell.execute_reply": "2023-08-18T07:07:32.723104Z"
        },
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "b8493d0a",
        "outputId": "b4c77906-fe3e-4680-ac27-9f57e8f1f596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x.grad == 4 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2733c623",
      "metadata": {
        "origin_pos": 25,
        "id": "2733c623"
      },
      "source": [
        "[**现在计算`x`的另一个函数。**]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f2fcd392",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.729368Z",
          "iopub.status.busy": "2023-08-18T07:07:32.728433Z",
          "iopub.status.idle": "2023-08-18T07:07:32.736493Z",
          "shell.execute_reply": "2023-08-18T07:07:32.735715Z"
        },
        "origin_pos": 27,
        "tab": [
          "pytorch"
        ],
        "id": "f2fcd392",
        "outputId": "b1e16842-d231-4ba0-b77a-2046b4c7a326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\n",
        "x.grad.zero_()\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f4f459",
      "metadata": {
        "origin_pos": 30,
        "id": "58f4f459"
      },
      "source": [
        "## 非标量变量的反向传播\n",
        "\n",
        "当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。\n",
        "对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n",
        "\n",
        "然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括[**深度学习中**]），\n",
        "但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。\n",
        "这里(**，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e62a5d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.740109Z",
          "iopub.status.busy": "2023-08-18T07:07:32.739419Z",
          "iopub.status.idle": "2023-08-18T07:07:32.745803Z",
          "shell.execute_reply": "2023-08-18T07:07:32.744893Z"
        },
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "f4e62a5d",
        "outputId": "303eb0a0-28de-4287-b1ea-d82434970416"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n",
        "# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\n",
        "x.grad.zero_()\n",
        "y = x * x\n",
        "# 等价于y.backward(torch.ones(len(x)))\n",
        "y.sum().backward()\n",
        "x.grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f510c4",
      "metadata": {
        "origin_pos": 35,
        "id": "80f510c4"
      },
      "source": [
        "## 分离计算\n",
        "\n",
        "有时，我们希望[**将某些计算移动到记录的计算图之外**]。\n",
        "例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。\n",
        "想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数，\n",
        "并且只考虑到`x`在`y`被计算后发挥的作用。\n",
        "\n",
        "这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值，\n",
        "但丢弃计算图中如何计算`y`的任何信息。\n",
        "换句话说，梯度不会向后流经`u`到`x`。\n",
        "因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理，\n",
        "而不是`z=x*x*x`关于`x`的偏导数。\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x  #x是一个向量"
      ],
      "metadata": {
        "id": "kqfqJMC6yqTU",
        "outputId": "6c59b3f0-aa0e-477f-e12c-881eb4a4ce53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kqfqJMC6yqTU",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8dab493d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.749398Z",
          "iopub.status.busy": "2023-08-18T07:07:32.748759Z",
          "iopub.status.idle": "2023-08-18T07:07:32.755280Z",
          "shell.execute_reply": "2023-08-18T07:07:32.754543Z"
        },
        "origin_pos": 37,
        "tab": [
          "pytorch"
        ],
        "id": "8dab493d",
        "outputId": "4985ff29-746c-45e8-e256-560468d8a334",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "x.grad.zero_()  #梯度清零\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "id": "G9s1TwrXy0ve",
        "outputId": "8d8cbab8-40e6-4d66-c42d-26d5d3d63437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "G9s1TwrXy0ve",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 4., 9.])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fe6f9c",
      "metadata": {
        "origin_pos": 40,
        "id": "f8fe6f9c"
      },
      "source": [
        "由于记录了`y`的计算结果，我们可以随后在`y`上调用反向传播，\n",
        "得到`y=x*x`关于的`x`的导数，即`2*x`。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "271a9b3a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.759344Z",
          "iopub.status.busy": "2023-08-18T07:07:32.758633Z",
          "iopub.status.idle": "2023-08-18T07:07:32.764663Z",
          "shell.execute_reply": "2023-08-18T07:07:32.763922Z"
        },
        "origin_pos": 42,
        "tab": [
          "pytorch"
        ],
        "id": "271a9b3a",
        "outputId": "4ceeb403-e11f-4ddc-9b86-8df0516afd04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd79d12f",
      "metadata": {
        "origin_pos": 45,
        "id": "fd79d12f"
      },
      "source": [
        "## Python控制流的梯度计算\n",
        "\n",
        "使用自动微分的一个好处是：\n",
        "[**即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度**]。\n",
        "在下面的代码中，`while`循环的迭代次数和`if`语句的结果都取决于输入`a`的值。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6323b2ff",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.769249Z",
          "iopub.status.busy": "2023-08-18T07:07:32.768616Z",
          "iopub.status.idle": "2023-08-18T07:07:32.773175Z",
          "shell.execute_reply": "2023-08-18T07:07:32.772293Z"
        },
        "origin_pos": 47,
        "tab": [
          "pytorch"
        ],
        "id": "6323b2ff"
      },
      "outputs": [],
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51aaf333",
      "metadata": {
        "origin_pos": 50,
        "id": "51aaf333"
      },
      "source": [
        "让我们计算梯度。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7719d6b6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.777740Z",
          "iopub.status.busy": "2023-08-18T07:07:32.777207Z",
          "iopub.status.idle": "2023-08-18T07:07:32.782254Z",
          "shell.execute_reply": "2023-08-18T07:07:32.781458Z"
        },
        "origin_pos": 52,
        "tab": [
          "pytorch"
        ],
        "id": "7719d6b6"
      },
      "outputs": [],
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a,d"
      ],
      "metadata": {
        "id": "cTpGMJ5ezSf9",
        "outputId": "89e0c7b5-5c03-4cf1-c0f7-4fc041fdcad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cTpGMJ5ezSf9",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-1.0723, requires_grad=True),\n",
              " tensor(-109804.1484, grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad"
      ],
      "metadata": {
        "id": "zrODh6tHzY3Q",
        "outputId": "e70f0686-c5b0-4eb2-daf6-d7a98ac87a5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zrODh6tHzY3Q",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(102400.)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "816a1ac2",
      "metadata": {
        "origin_pos": 55,
        "id": "816a1ac2"
      },
      "source": [
        "我们现在可以分析上面定义的`f`函数。\n",
        "请注意，它在其输入`a`中是分段线性的。\n",
        "换言之，对于任何`a`，存在某个常量标量`k`，使得`f(a)=k*a`，其中`k`的值取决于输入`a`，因此可以用`d/a`验证梯度是否正确。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2595bdc0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T07:07:32.785728Z",
          "iopub.status.busy": "2023-08-18T07:07:32.785179Z",
          "iopub.status.idle": "2023-08-18T07:07:32.790672Z",
          "shell.execute_reply": "2023-08-18T07:07:32.789892Z"
        },
        "origin_pos": 57,
        "tab": [
          "pytorch"
        ],
        "id": "2595bdc0",
        "outputId": "4888c28c-5445-4378-ff45-0f7c3c2e46a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "a.grad == d / a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 二阶导数的计算，使用auto_grad"
      ],
      "metadata": {
        "id": "QIrZa9pb1s1f"
      },
      "id": "QIrZa9pb1s1f"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.arange(4.0, requires_grad=True)\n",
        "y=x*x*x.sum()\n",
        "x,y"
      ],
      "metadata": {
        "id": "hnnMb6WsztMY",
        "outputId": "6a82c873-704d-44d7-c90b-68ea827ea96b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hnnMb6WsztMY",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
              " tensor([ 0.,  6., 24., 54.], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dy_dx=torch.autograd.grad(y.sum(),x,create_graph=True)[0]\n",
        "print('一阶导数为:',dy_dx)"
      ],
      "metadata": {
        "id": "fCyU59283g4X",
        "outputId": "4c79c1c8-79b6-434f-a979-9053d5fae499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fCyU59283g4X",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "一阶导数为: tensor([14., 26., 38., 50.], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d2y_dx2 = torch.autograd.grad(dy_dx.sum(), x)[0]\n",
        "print(\"二阶导数 d²y/dx² =\", d2y_dx2)"
      ],
      "metadata": {
        "id": "cz0RY6Nf3YdS",
        "outputId": "f36e0edb-b019-4a8e-af21-39bb89315802",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cz0RY6Nf3YdS",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "二阶导数 d²y/dx² = tensor([24., 32., 40., 48.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67fb5517",
      "metadata": {
        "origin_pos": 60,
        "id": "67fb5517"
      },
      "source": [
        "## 小结\n",
        "\n",
        "* 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n",
        "\n",
        "## 练习\n",
        "\n",
        "1. 为什么计算二阶导数比一阶导数的开销要更大？\n",
        "1. 在运行反向传播函数之后，立即再次运行它，看看会发生什么。\n",
        "1. 在控制流的例子中，我们计算`d`关于`a`的导数，如果将变量`a`更改为随机向量或矩阵，会发生什么？\n",
        "1. 重新设计一个求控制流梯度的例子，运行并分析结果。\n",
        "1. 使$f(x)=\\sin(x)$，绘制$f(x)$和$\\frac{df(x)}{dx}$的图像，其中后者不使用$f'(x)=\\cos(x)$。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "530f74f8",
      "metadata": {
        "origin_pos": 62,
        "tab": [
          "pytorch"
        ],
        "id": "530f74f8"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1759)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}